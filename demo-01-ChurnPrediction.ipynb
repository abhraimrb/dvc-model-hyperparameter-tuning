{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59bcae58",
   "metadata": {},
   "source": [
    "We can repeat the steps from Regression Project..We have to  rename the project directory name and the git remote configuration should be remote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390d916b",
   "metadata": {},
   "source": [
    "Installing packages required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ab0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf68b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_churn_data = pd.read_csv('data/CustomerTravel.csv')\n",
    "\n",
    "travel_churn_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0317c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_churn_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ccdf29",
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_churn_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c37cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_churn_data = travel_churn_data.drop_duplicates()\n",
    "travel_churn_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51e7dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_churn_data['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2046c475",
   "metadata": {},
   "outputs": [],
   "source": [
    "travel_churn_data['AnnualIncomeClass'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d6576f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {'Low Income': 0,'Middle Income': 1, 'High Income': 2}\n",
    "\n",
    "travel_churn_data['AnnualIncomeClass'] = travel_churn_data['AnnualIncomeClass'].replace(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66746c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['FrequentFlyer','AccountSyncedToSocialMedia', 'BookedHotelOrNot']\n",
    "categorical_transformer = Pipeline(steps=[('encoder', OneHotEncoder(handle_unknown = 'ignore', drop = 'first'))])\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers = [('cat_tr', categorical_transformer, categorical_features)], \n",
    "                                 remainder = StandardScaler())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5fc072",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = travel_churn_data.drop(labels = ['Target'], axis = 1)\n",
    " \n",
    "y = travel_churn_data['Target']\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 124)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4056b4d",
   "metadata": {},
   "source": [
    "Logistic Regression Model is trained and parameters and metrics are logged.\n",
    "We can display the display the logged parameters and metrics inside the notebook using report='notebook'.\n",
    "We can generate markdown reports also using report='md'. First of all, we will run LR model with default parameters. Then we will set the class_weight parameter to balanced.When we do metrics comparison, Recall score is improved for 'balanced' LR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a189eed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from dvclive import Live\n",
    "lr_model = LogisticRegression()\n",
    "#lr_model = LogisticRegression(class_weight = 'balanced')\n",
    "    \n",
    "pipe_lr = Pipeline(steps = [('preprocessor', preprocessor), ('classifier', lr_model)])\n",
    "model_params = lr_model.get_params()\n",
    "print(model_params)\n",
    "pipe_lr.fit(X_train, y_train)\n",
    " \n",
    "predictions = pipe_lr.predict(X_test)\n",
    "predictions_predict_prob = pipe_lr.predict_proba(X_test)\n",
    "    \n",
    "train_accuracy_score = pipe_lr.score(X_train, y_train)\n",
    "test_accuracy_score = accuracy_score(y_test, predictions)\n",
    "test_precision_score = precision_score(y_test, predictions)\n",
    "test_recall_score = recall_score(y_test, predictions)\n",
    "test_f1_score = f1_score(y_test, predictions)\n",
    "auc_score = roc_auc_score(y_test,  predictions_predict_prob[:, 1])\n",
    "    \n",
    "metrics = {\n",
    "        'Train_accuracy_score': train_accuracy_score, \n",
    "        'Test_accuracy_score': test_accuracy_score,\n",
    "        'Test_precision_score': test_precision_score,\n",
    "        'Test_recall_score': test_recall_score,\n",
    "        'Test_f1_score': test_f1_score,\n",
    "        'AUC_score': auc_score }\n",
    "    \n",
    "with Live(save_dvc_exp = True, report = 'notebook', exp_message = 'LogisticRegressionClassifier') as live:\n",
    "    live.log_params(model_params)\n",
    "        \n",
    "    live.log_metric('Train_accuracy_score',train_accuracy_score)\n",
    "    live.log_metric('Test_accuracy_score',test_accuracy_score)\n",
    "    live.log_metric('Test_precision_score', test_precision_score)\n",
    "    live.log_metric('Test_recall_score', test_recall_score)\n",
    "    live.log_metric('Test_f1_score', test_f1_score)\n",
    "    live.log_metric('AUC_score', auc_score)\n",
    "    \n",
    "    live.log_sklearn_plot('confusion_matrix', y_test, predictions, name = 'cm.json')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725ef635",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lr_model = LogisticRegression()\n",
    "lr_model = LogisticRegression(class_weight = 'balanced')\n",
    "    \n",
    "pipe_lr = Pipeline(steps = [('preprocessor', preprocessor), ('classifier', lr_model)])\n",
    "model_params = lr_model.get_params()\n",
    "print(model_params)\n",
    "pipe_lr.fit(X_train, y_train)\n",
    " \n",
    "predictions = pipe_lr.predict(X_test)\n",
    "predictions_predict_prob = pipe_lr.predict_proba(X_test)\n",
    "    \n",
    "train_accuracy_score = pipe_lr.score(X_train, y_train)\n",
    "test_accuracy_score = accuracy_score(y_test, predictions)\n",
    "test_precision_score = precision_score(y_test, predictions)\n",
    "test_recall_score = recall_score(y_test, predictions)\n",
    "test_f1_score = f1_score(y_test, predictions)\n",
    "auc_score = roc_auc_score(y_test,  predictions_predict_prob[:, 1])\n",
    "    \n",
    "metrics = {\n",
    "        'Train_accuracy_score': train_accuracy_score, \n",
    "        'Test_accuracy_score': test_accuracy_score,\n",
    "        'Test_precision_score': test_precision_score,\n",
    "        'Test_recall_score': test_recall_score,\n",
    "        'Test_f1_score': test_f1_score,\n",
    "        'AUC_score': auc_score }\n",
    "    \n",
    "with Live(save_dvc_exp = True, exp_message = 'LogisticRegressionClassifier_balanced_class_weight') as live:\n",
    "    live.log_params(model_params)\n",
    "        \n",
    "    live.log_metric('Train_accuracy_score',train_accuracy_score)\n",
    "    live.log_metric('Test_accuracy_score',test_accuracy_score)\n",
    "    live.log_metric('Test_precision_score', test_precision_score)\n",
    "    live.log_metric('Test_recall_score', test_recall_score)\n",
    "    live.log_metric('Test_f1_score', test_f1_score)\n",
    "    live.log_metric('AUC_score', auc_score)\n",
    "    \n",
    "    live.log_sklearn_plot('confusion_matrix', y_test, predictions, name = 'cm.json')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b24eb2",
   "metadata": {},
   "source": [
    "RandomForest Model is trained and parameters and metrics are logged.It can easily seen that this experiment's performance is quite better than Logistic Regression Experiment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc2ff22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier()\n",
    "#rf_model = RandomForestClassifier(class_weight = 'balanced')\n",
    "    \n",
    "pipe_rf = Pipeline(steps = [('preprocessor', preprocessor), ('classifier', rf_model)])\n",
    "model_params = rf_model.get_params()\n",
    "print(model_params)\n",
    "pipe_rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = pipe_rf.predict(X_test)\n",
    "predictions_predict_prob = pipe_rf.predict_proba(X_test)\n",
    "    \n",
    "train_accuracy_score = pipe_rf.score(X_train, y_train)\n",
    "test_accuracy_score = accuracy_score(y_test, predictions)\n",
    "test_precision_score = precision_score(y_test, predictions)\n",
    "test_recall_score = recall_score(y_test, predictions)\n",
    "test_f1_score = f1_score(y_test, predictions)\n",
    "auc_score = roc_auc_score(y_test,  predictions_predict_prob[:, 1])\n",
    "    \n",
    "metrics = {\n",
    "        'Train_accuracy_score': train_accuracy_score, \n",
    "        'Test_accuracy_score': test_accuracy_score,\n",
    "        'Test_precision_score': test_precision_score,\n",
    "        'Test_recall_score': test_recall_score,\n",
    "        'Test_f1_score': test_f1_score,\n",
    "        'AUC_score': auc_score }\n",
    "   \n",
    "with Live(save_dvc_exp = True, exp_message = 'RFClassifier') as live:\n",
    "    live.log_params(model_params)\n",
    "        \n",
    "    live.log_metric('Train_accuracy_score',train_accuracy_score)\n",
    "    live.log_metric('Test_accuracy_score',test_accuracy_score)\n",
    "    live.log_metric('Test_precision_score', test_precision_score)\n",
    "    live.log_metric('Test_recall_score', test_recall_score)\n",
    "    live.log_metric('Test_f1_score', test_f1_score)\n",
    "    live.log_metric('AUC_score', auc_score)\n",
    "    live.log_sklearn_plot('confusion_matrix', y_test, predictions, name = 'cm.json')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab746da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "#rf_model = RandomForestClassifier()\n",
    "rf_model = RandomForestClassifier(class_weight = 'balanced')\n",
    "    \n",
    "pipe_rf = Pipeline(steps = [('preprocessor', preprocessor), ('classifier', rf_model)])\n",
    "model_params = rf_model.get_params()\n",
    "print(model_params)\n",
    "pipe_rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = pipe_rf.predict(X_test)\n",
    "predictions_predict_prob = pipe_rf.predict_proba(X_test)\n",
    "    \n",
    "train_accuracy_score = pipe_rf.score(X_train, y_train)\n",
    "test_accuracy_score = accuracy_score(y_test, predictions)\n",
    "test_precision_score = precision_score(y_test, predictions)\n",
    "test_recall_score = recall_score(y_test, predictions)\n",
    "test_f1_score = f1_score(y_test, predictions)\n",
    "auc_score = roc_auc_score(y_test,  predictions_predict_prob[:, 1])\n",
    "    \n",
    "metrics = {\n",
    "        'Train_accuracy_score': train_accuracy_score, \n",
    "        'Test_accuracy_score': test_accuracy_score,\n",
    "        'Test_precision_score': test_precision_score,\n",
    "        'Test_recall_score': test_recall_score,\n",
    "        'Test_f1_score': test_f1_score,\n",
    "        'AUC_score': auc_score }\n",
    "   \n",
    "with Live(save_dvc_exp = True, exp_message = 'RFClassifier_balanced_class_weight') as live:\n",
    "    live.log_params(model_params)\n",
    "        \n",
    "    live.log_metric('Train_accuracy_score',train_accuracy_score)\n",
    "    live.log_metric('Test_accuracy_score',test_accuracy_score)\n",
    "    live.log_metric('Test_precision_score', test_precision_score)\n",
    "    live.log_metric('Test_recall_score', test_recall_score)\n",
    "    live.log_metric('Test_f1_score', test_f1_score)\n",
    "    live.log_metric('AUC_score', auc_score)\n",
    "    live.log_sklearn_plot('confusion_matrix', y_test, predictions, name = 'cm.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07b36e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_model = RandomForestClassifier()\n",
    "#rf_model = RandomForestClassifier(class_weight = 'balanced')\n",
    "    \n",
    "pipe_rf = Pipeline(steps = [('preprocessor', preprocessor), ('classifier', rf_model)])\n",
    "model_params = rf_model.get_params()\n",
    "print(model_params)\n",
    "pipe_rf.fit(X_train, y_train)\n",
    "\n",
    "predictions = pipe_rf.predict(X_test)\n",
    "predictions_predict_prob = pipe_rf.predict_proba(X_test)\n",
    "    \n",
    "train_accuracy_score = pipe_rf.score(X_train, y_train)\n",
    "test_accuracy_score = accuracy_score(y_test, predictions)\n",
    "test_precision_score = precision_score(y_test, predictions)\n",
    "test_recall_score = recall_score(y_test, predictions)\n",
    "test_f1_score = f1_score(y_test, predictions)\n",
    "auc_score = roc_auc_score(y_test,  predictions_predict_prob[:, 1])\n",
    "    \n",
    "metrics = {\n",
    "        'Train_accuracy_score': train_accuracy_score, \n",
    "        'Test_accuracy_score': test_accuracy_score,\n",
    "        'Test_precision_score': test_precision_score,\n",
    "        'Test_recall_score': test_recall_score,\n",
    "        'Test_f1_score': test_f1_score,\n",
    "        'AUC_score': auc_score }\n",
    "   \n",
    "with Live(save_dvc_exp = True, exp_message = 'RFClassifier_default_params') as live:\n",
    "    live.log_params(model_params)\n",
    "        \n",
    "    live.log_metric('Train_accuracy_score',train_accuracy_score)\n",
    "    live.log_metric('Test_accuracy_score',test_accuracy_score)\n",
    "    live.log_metric('Test_precision_score', test_precision_score)\n",
    "    live.log_metric('Test_recall_score', test_recall_score)\n",
    "    live.log_metric('Test_f1_score', test_f1_score)\n",
    "    live.log_metric('AUC_score', auc_score)\n",
    "    live.log_sklearn_plot('confusion_matrix', y_test, predictions, name = 'cm.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7012dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svc_model = SVC(class_weight = 'balanced')\n",
    "    \n",
    "pipe_svc = Pipeline(steps = [('preprocessor', preprocessor), ('classifier', svc_model )])\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "model_params = svc_model.get_params()\n",
    "print(model_params)\n",
    "predictions =  pipe_svc.predict(X_test)\n",
    "    \n",
    "train_accuracy_score = pipe_svc.score(X_train, y_train)\n",
    "test_accuracy_score = accuracy_score(y_test, predictions)\n",
    "test_precision_score = precision_score(y_test, predictions)\n",
    "test_recall_score = recall_score(y_test, predictions)\n",
    "test_f1_score = f1_score(y_test, predictions)\n",
    "    \n",
    "metrics = {\n",
    "        'Train_accuracy_score': train_accuracy_score, \n",
    "        'Test_accuracy_score': test_accuracy_score,\n",
    "        'Test_precision_score': test_precision_score,\n",
    "        'Test_recall_score': test_recall_score,\n",
    "        'Test_f1_score': test_f1_score}\n",
    "   \n",
    "with Live(save_dvc_exp = True, exp_message = 'SVClassifier_class_weight_balanced') as live:\n",
    "    live.log_params(model_params)\n",
    "        \n",
    "    live.log_metric('Train_accuracy_score', train_accuracy_score)\n",
    "    live.log_metric('Test_accuracy_score', test_accuracy_score)\n",
    "    live.log_metric('Test_precision_score', test_precision_score)\n",
    "    live.log_metric('Test_recall_score', test_recall_score)\n",
    "    live.log_metric('Test_f1_score', test_f1_score)\n",
    "    live.log_sklearn_plot('confusion_matrix', y_test, predictions, name = 'cm.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6491ae3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c4b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "#xgb_model = XGBClassifier()\n",
    "xgb_model = XGBClassifier(scale_pos_weight=2)\n",
    "    \n",
    "pipe_xgb = Pipeline(steps = [('preprocessor', preprocessor), ('classifier', xgb_model )])\n",
    "pipe_xgb.fit(X_train, y_train)\n",
    "model_params = xgb_model.get_params()\n",
    "print(model_params)\n",
    "predictions =  pipe_xgb.predict(X_test)\n",
    "predictions_predict_prob = pipe_xgb.predict_proba(X_test)\n",
    "    \n",
    "train_accuracy_score = pipe_xgb.score(X_train, y_train)\n",
    "test_accuracy_score = accuracy_score(y_test, predictions)\n",
    "test_precision_score = precision_score(y_test, predictions)\n",
    "test_recall_score = recall_score(y_test, predictions)\n",
    "test_f1_score = f1_score(y_test, predictions)\n",
    "auc_score = roc_auc_score(y_test,  predictions_predict_prob[:, 1])\n",
    "    \n",
    "metrics = {\n",
    "        'Train_accuracy_score': train_accuracy_score, \n",
    "        'Test_accuracy_score': test_accuracy_score,\n",
    "        'Test_precision_score': test_precision_score,\n",
    "        'Test_recall_score': test_recall_score,\n",
    "        'Test_f1_score': test_f1_score,\n",
    "        'AUC_score': auc_score }\n",
    "   \n",
    "with Live(save_dvc_exp = True, exp_message = 'XGBoostingClassifier') as live:\n",
    "    live.log_params(model_params)\n",
    "        \n",
    "    live.log_metric('Train_accuracy_score', train_accuracy_score)\n",
    "    live.log_metric('Test_accuracy_score', test_accuracy_score)\n",
    "    live.log_metric('Test_precision_score', test_precision_score)\n",
    "    live.log_metric('Test_recall_score', test_recall_score)\n",
    "    live.log_metric('Test_f1_score', test_f1_score)\n",
    "    live.log_metric('AUC_score', auc_score)\n",
    "    live.log_sklearn_plot('confusion_matrix', y_test, predictions, name = 'cm.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717f8b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dvc.api\n",
    "\n",
    "exps = dvc.api.exp_show('CustomerChurnPredictionProject.git')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf80369",
   "metadata": {},
   "outputs": [],
   "source": [
    "exps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21080bf6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
